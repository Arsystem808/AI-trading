name: Reusable ETL

on:
  workflow_call:
    inputs:
      env:
        type: string
        required: false
        default: "dev"
      command:
        type: string
        required: false
        default: "python -m core.etl.transform"
      workdir:
        type: string
        required: false
        default: "."
      tickers:
        type: string
        required: false
        default: "AAPL,MSFT,TSLA"
    secrets:
      POLYGON_API_KEY:
        required: true
        description: 'Polygon.io API key for market data'

permissions:
  contents: read

defaults:
  run:
    shell: bash

jobs:
  etl:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # âœ… Ð—Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ Ð·Ð°Ð²Ð¸ÑÑˆÐ¸Ñ… jobs
    env:
      ENV: ${{ inputs.env }}
      WORKDIR_IN: ${{ inputs.workdir }}
      POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # âœ… Ð‘Ñ‹ÑÑ‚Ñ€ÐµÐµ Ð´Ð»Ñ ETL

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: '**/requirements*.txt'

      - name: Validate secrets
        run: |
          set -euo pipefail
          if [ -z "${POLYGON_API_KEY:-}" ]; then
            echo "::error::POLYGON_API_KEY is not set"
            exit 1
          fi
          echo "::add-mask::$POLYGON_API_KEY"
          echo "âœ… Secrets validated"

      - name: Compute paths
        run: |
          set -euo pipefail
          WD="${WORKDIR_IN:-.}"
          if [ "$WD" = "." ]; then
            BASE="${GITHUB_WORKSPACE}"
          else
            BASE="${GITHUB_WORKSPACE}/${WD#./}"
          fi
          ART_DIR="${BASE}/artifacts"
          echo "WORKDIR_NORM=${WD}" >> "$GITHUB_ENV"
          echo "ARTIFACTS_DIR=${ART_DIR}" >> "$GITHUB_ENV"
          mkdir -p "${ART_DIR}"
          echo "ðŸ“‚ Working directory: ${BASE}"
          echo "ðŸ“¦ Artifacts directory: ${ART_DIR}"

      - name: Cache market data
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/polygon_data
            data/cache
          key: polygon-cache-${{ inputs.tickers }}-${{ github.run_id }}
          restore-keys: |
            polygon-cache-${{ inputs.tickers }}-
            polygon-cache-

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel
          if [ -f requirements.txt ]; then 
            pip install -r requirements.txt
          fi
          pip install pyarrow pandas numpy
        working-directory: ${{ env.WORKDIR_NORM }}

      - name: Run ETL
        id: etl
        env:
          ARTIFACTS_DIR: ${{ env.ARTIFACTS_DIR }}
        run: |
          set -euo pipefail
          # Validate command safety
          CMD="${{ inputs.command }}"
          if [[ ! "$CMD" =~ ^python\ -m\ core\. ]]; then
            echo "::error::Only 'python -m core.*' commands allowed"
            exit 1
          fi
          
          echo "ðŸš€ Running ETL: $CMD"
          $CMD --out "${ARTIFACTS_DIR}/latest.parquet" || {
            echo "::error::ETL command failed with exit code $?"
            exit 1
          }
          echo "âœ… ETL completed successfully"
        working-directory: ${{ env.WORKDIR_NORM }}

      - name: Collect outputs
        if: success()  # âœ… Ð¢Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ETL ÑƒÑÐ¿ÐµÑˆÐµÐ½
        env:
          ARTIFACTS_DIR: ${{ env.ARTIFACTS_DIR }}
        run: |
          set -euo pipefail
          mkdir -p "${ARTIFACTS_DIR}"
          
          # Collect standard directories
          for d in logs output data; do
            if [ -d "$d" ] && [ -n "$(ls -A $d)" ]; then
              echo "ðŸ“‚ Collecting $d/"
              rsync -a "$d"/ "${ARTIFACTS_DIR}/$d"/
            fi
          done
          
          # Generate metadata
          cat > "${ARTIFACTS_DIR}/run_metadata.json" <<EOF
          {
            "run_id": "${GITHUB_RUN_ID}",
            "run_number": "${GITHUB_RUN_NUMBER}",
            "environment": "${ENV}",
            "timestamp": "$(date -Is)",
            "repository": "${GITHUB_REPOSITORY}",
            "commit_sha": "${GITHUB_SHA}",
            "actor": "${GITHUB_ACTOR}"
          }
          EOF
          
          echo "âœ… Metadata generated"
        working-directory: ${{ env.WORKDIR_NORM }}

      - name: Validate artifacts
        if: success()
        env:
          ARTIFACTS_DIR: ${{ env.ARTIFACTS_DIR }}
        run: |
          set -euo pipefail
          echo "ðŸ“Š Artifacts summary:"
          
          # Check for parquet files
          PARQUET_COUNT=$(find "${ARTIFACTS_DIR}" -name "*.parquet" | wc -l)
          echo "  Parquet files: ${PARQUET_COUNT}"
          
          if [ "$PARQUET_COUNT" -eq 0 ]; then
            echo "::warning::No parquet files generated"
          fi
          
          # List all files with sizes
          echo ""
          echo "ðŸ“„ File listing:"
          find "${ARTIFACTS_DIR}" -type f -exec ls -lh {} \; | awk '{print "  " $9 " (" $5 ")"}'
          
          # Total size
          TOTAL_SIZE=$(du -sh "${ARTIFACTS_DIR}" | awk '{print $1}')
          echo ""
          echo "ðŸ’¾ Total size: ${TOTAL_SIZE}"

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: etl-artifacts-${{ inputs.env }}-${{ github.run_number }}
          path: ${{ env.ARTIFACTS_DIR }}/
          if-no-files-found: warn  # âœ… ÐÐµ ÑÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ ETL
          retention-days: 7  # âœ… ÐÐ²Ñ‚Ð¾ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð²
          include-hidden-files: false  # âœ… Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½ÐµÐµ
          compression-level: 6  # âœ… Ð‘Ð°Ð»Ð°Ð½Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸/Ñ€Ð°Ð·Ð¼ÐµÑ€Ð°
